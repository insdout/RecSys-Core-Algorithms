{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering (NCF) and Neural Matrix Factorization (NeuMF) Recommendation Systems\n",
    "\n",
    "## Overview\n",
    "\n",
    "Collaborative Filtering is a popular technique in recommendation systems that leverages user-item interactions to make personalized recommendations. Neural Collaborative Filtering (NCF) and Neural Matrix Factorization (NeuMF) are advanced models that incorporate neural networks to enhance collaborative filtering.\n",
    "\n",
    "## Neural Collaborative Filtering (NCF)\n",
    "\n",
    "NCF is a collaborative filtering model that integrates neural networks into the traditional collaborative filtering framework. It is designed to capture complex patterns and non-linear relationships in user-item interactions. NCF utilizes neural networks to model both user and item embeddings, combining the strengths of collaborative filtering and deep learning.\n",
    "\n",
    "[Link to NCF and NeuMF Paper](papers/NeuMF.pdf)\n",
    "\n",
    "### Architecture\n",
    "\n",
    "NCF typically consists of the following components:\n",
    "- **Embedding Layers:** Embeds users and items into low-dimensional latent vectors.\n",
    "- **Neural Network Layers:** Processes the embedded vectors to capture non-linear patterns.\n",
    "- **Output Layer:** Generates a prediction score indicating the likelihood of user-item interactions.\n",
    "\n",
    "### Key Advantages\n",
    "- **Flexibility:** NCF can capture intricate user-item relationships, including implicit feedback.\n",
    "- **Scalability:** The neural network structure allows for scalability with large datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"assets/NCF.png\" alt=\"NCF\" width=\"70%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, file_path='datasets/ml-1m/movies.dat', test_size=0.2):\n",
    "        # Define the column names\n",
    "        columns = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "        self.test_size = test_size\n",
    "        # Read the .dat file using pandas\n",
    "        self.ratings = pd.read_csv(file_path, sep='::', header=None, names=columns, engine='python', encoding='latin-1')\n",
    "        self.ratings['timestamp'] = pd.to_datetime(self.ratings['timestamp'], unit='s')\n",
    "        self.user_mapping = {user_id: idx for idx, user_id in enumerate(self.ratings['userId'].unique())}\n",
    "        self.reverse_user_mapping = {idx: user_id for idx, user_id in enumerate(self.ratings['userId'].unique())}\n",
    "        self.movie_mapping = {movie_id: idx for idx, movie_id in enumerate(self.ratings['movieId'].unique())}\n",
    "        self.reverse_movie_mapping = {idx: movie_id for idx, movie_id in enumerate(self.ratings['movieId'].unique())}\n",
    "        self.map_user_movie_ids()\n",
    "\n",
    "    def train_test_split(self):\n",
    "        # Sort the DataFrame by the 'timestamp' column\n",
    "        df_sorted = self.ratings.sort_values(by='timestamp')\n",
    "\n",
    "        train_ratings, test_ratings = [], []\n",
    "\n",
    "        for _, user_data in df_sorted.groupby('userId'):\n",
    "            n_samples = len(user_data)\n",
    "            num_test_samples = int(self.test_size * n_samples)\n",
    "            if num_test_samples:\n",
    "                train_ratings.append(user_data.iloc[:-num_test_samples])\n",
    "                test_ratings.append(user_data.iloc[-num_test_samples:])\n",
    "            else:\n",
    "                train_ratings.append(user_data)\n",
    "\n",
    "        train_ratings = pd.concat(train_ratings)\n",
    "        test_ratings = pd.concat(test_ratings)\n",
    "        return train_ratings, test_ratings, self.ratings\n",
    "\n",
    "    def map_user_movie_ids(self):\n",
    "        # Map userId and movieId to start from 0 and be incremental\n",
    "        user_mapping = {user_id: idx for idx, user_id in enumerate(self.ratings['userId'].unique())}\n",
    "        movie_mapping = {movie_id: idx for idx, movie_id in enumerate(self.ratings['movieId'].unique())}\n",
    "\n",
    "        self.ratings['userId'] = self.ratings['userId'].map(user_mapping)\n",
    "        self.ratings['movieId'] = self.ratings['movieId'].map(movie_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Replace 'your_dataset.dat' with the actual file path\n",
    "data_preprocessor = DataPreprocessor('datasets/ml-1m/ratings.dat')\n",
    "train_ratings, test_ratings, ratings = data_preprocessor.train_test_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(802553, 4) (197656, 4)\n",
      "3706 3705\n",
      "6040 6039\n",
      "   userId  movieId  rating           timestamp\n",
      "0       0        0       5 2000-12-31 22:12:40\n",
      "1       0        1       3 2000-12-31 22:35:09\n",
      "2       0        2       3 2000-12-31 22:32:48\n",
      "3       0        3       4 2000-12-31 22:04:35\n",
      "4       0        4       5 2001-01-06 23:38:11\n"
     ]
    }
   ],
   "source": [
    "# Display the preprocessed DataFrame\n",
    "print(train_ratings.shape, test_ratings.shape)\n",
    "print(len(data_preprocessor.ratings['movieId'].unique()), max(data_preprocessor.ratings['movieId'].unique()))\n",
    "print(len(data_preprocessor.ratings['userId'].unique()), max(data_preprocessor.ratings['userId'].unique()))\n",
    "print(data_preprocessor.ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosDataset(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        self.ratings = ratings[['userId','movieId']].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Placeholder, modify according to your needs\n",
    "        user_id, movie_id = self.ratings[idx]\n",
    "        return {'user_id': user_id, 'item_id': movie_id, 'target': 1}\n",
    "    \n",
    "class NegativeSampler(Dataset):\n",
    "    def __init__(self, ratings, negative_ratio=5, strategy='uniform'):\n",
    "        self.ratings = ratings\n",
    "        self.strategy = strategy\n",
    "        self.negative_ratio = negative_ratio\n",
    "        self.user_ids = self.ratings['userId'].unique()\n",
    "        self.item_ids = self.ratings['movieId'].unique()\n",
    "        self.user_item_set = set(zip(ratings['userId'], ratings['movieId']))\n",
    "        if self.strategy == 'uniform':\n",
    "            self.user_freq = None\n",
    "            self.item_freq = None\n",
    "        else:\n",
    "            self.user_freq = self._get_user_freq()\n",
    "            self.item_freq = self._get_item_freq()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.negative_ratio*len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Placeholder, modify according to your needs\n",
    "        neg_user_id, neg_movie_id = self.negative_sample()\n",
    "        return {'user_id': neg_user_id, 'item_id': neg_movie_id, 'target': 0}\n",
    "\n",
    "    def negative_sample(self):\n",
    "        # Sample negative items\n",
    "        while True:\n",
    "            if self.strategy == 'uniform':\n",
    "                user_id = np.random.choice(self.user_ids)\n",
    "                neg_item = np.random.choice(self.item_ids)\n",
    "            else:\n",
    "                user_id = np.random.choice(self.user_ids, p=self.user_freq)\n",
    "                neg_item = np.random.choice(self.item_ids, p=self.item_freq)\n",
    "            if (user_id, neg_item) not in self.user_item_set:\n",
    "                return user_id, neg_item\n",
    "    \n",
    "    def _get_user_freq(self):\n",
    "        user_counts = self.ratings.groupby('userId').count()['movieId']\n",
    "        user_probs = user_counts/user_counts.sum()\n",
    "        user_probs = user_probs.to_dict()\n",
    "        user_probs_list = [user_probs[id] for id in self.user_ids]\n",
    "        return user_probs_list\n",
    "    \n",
    "    def _get_item_freq(self):\n",
    "        item_counts = self.ratings.groupby('movieId').count()['userId']\n",
    "        item_probs = item_counts/item_counts.sum()\n",
    "        item_probs = item_probs.to_dict()\n",
    "        item_probs_list = [item_probs[id] for id in self.item_ids]\n",
    "        return item_probs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, interactions_df, test_df):\n",
    "        self.interactions_df = interactions_df\n",
    "        self.test_df = test_df\n",
    "        self.users = interactions_df['userId'].unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.users[idx]\n",
    "        \n",
    "        # Get all interactions of the user\n",
    "        user_interactions = self.interactions_df[self.interactions_df['userId'] == user_id]\n",
    "        \n",
    "        # Extract the latest interactions as the test set\n",
    "        test_interaction = self.test_df[self.test_df['userId'] == user_id]\n",
    "        \n",
    "        # Sample 100 items that are not interacted by the user\n",
    "        all_items = self.interactions_df['movieId'].unique()\n",
    "        interacted_items = user_interactions['movieId'].unique()\n",
    "        non_interacted_items = np.setdiff1d(all_items, interacted_items)\n",
    "        sampled_items = np.random.choice(non_interacted_items, size=100, replace=False)\n",
    "        \n",
    "        # Create positive and negative samples for evaluation\n",
    " \n",
    "        #pos_array = np.stack([test_interaction['userId'].values, test_interaction['movieId'].values], axis=1)\n",
    "        #positive_sample = torch.tensor([pos_array], dtype=torch.long)\n",
    "        pos_user_ids = test_interaction['userId'].values.reshape(-1, 1)\n",
    "        pos_item_ids = test_interaction['movieId'].values.reshape(-1, 1)\n",
    "\n",
    "        #negative_samples = torch.tensor([[user_id, item] for item in sampled_items], dtype=torch.long)\n",
    "        neg_user_ids = np.array([user_id]*len(sampled_items)).reshape(-1, 1)\n",
    "        neg_item_ids = sampled_items.reshape(-1, 1)\n",
    "        sample_user_ids = np.vstack([pos_user_ids, neg_user_ids])\n",
    "        sample_movie_ids = np.vstack([pos_item_ids, neg_item_ids])\n",
    "        targets = np.vstack([np.ones_like(pos_user_ids), np.zeros_like(neg_user_ids)])\n",
    "        return {\n",
    "            'sample_userId': torch.tensor(sample_user_ids), \n",
    "            'sample_movieId': torch.tensor(sample_movie_ids), \n",
    "            'pos_movieId': torch.tensor(pos_item_ids),\n",
    "            'targets': torch.tensor(targets)\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Matrix Factorization (NeuMF)\n",
    "\n",
    "NeuMF is a hybrid recommendation model that combines the strengths of matrix factorization and neural networks. It is designed to improve the recommendation accuracy by leveraging both collaborative filtering and content-based features.\n",
    "\n",
    "[Link to NCF and NeuMF Paper](papers/NeuMF.pdf)\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The architecture of NeuMF includes:\n",
    "- **Matrix Factorization Component:** Learns user and item embeddings through traditional matrix factorization.\n",
    "- **Neural Network Component:** Learns additional non-linear patterns using neural networks.\n",
    "- **Final Prediction:** Combines predictions from both components to produce the final recommendation.\n",
    "\n",
    "### Key Advantages\n",
    "- **Hybrid Approach:** NeuMF combines collaborative filtering and neural networks, leveraging the benefits of both.\n",
    "- **Improved Accuracy:** The model captures both explicit and implicit feedback, enhancing recommendation accuracy.\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "NCF and NeuMF find applications in various domains, including:\n",
    "- **E-commerce:** Personalized product recommendations.\n",
    "- **Streaming Services:** Content recommendations for users.\n",
    "- **Social Networks:** Friend or content suggestions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"assets/NeuMF.png\" alt=\"NCF\" width=\"70%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, user_num, item_num, hidden_dims=[32, 16, 8]):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_emb = nn.Embedding(num_embeddings=user_num, embedding_dim=hidden_dims[0])\n",
    "        self.item_emb = nn.Embedding(num_embeddings=item_num, embedding_dim=hidden_dims[0])\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            layers.append(nn.Linear(in_features=hidden_dims[i], out_features=hidden_dims[i+1]))\n",
    "            layers.append(nn.Tanh())\n",
    "        layers.extend([nn.Linear(in_features=hidden_dims[-1], out_features=1), nn.Sigmoid()])\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_emb = self.user_emb(user_ids)\n",
    "        item_emb = self.item_emb(item_ids)\n",
    "        emb = user_emb*item_emb\n",
    "        out = self.mlp(emb)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 shape: torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4542],\n",
       "        [0.4361],\n",
       "        [0.4390]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = NCF(10, 10)\n",
    "\n",
    "x1 = torch.randint(low=0, high=10, size=(3, ))\n",
    "x2 = torch.randint(low=0, high=10, size=(3, ))\n",
    "t1 = torch.randint(low=0, high=1, size=(3, ))\n",
    "print(f'x1 shape: {x1.shape}')\n",
    "m(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max userId: 6040 Max movieId: 3706\n"
     ]
    }
   ],
   "source": [
    "n_items = len(ratings['movieId'].unique())\n",
    "n_users = len(ratings['userId'].unique())\n",
    "print(f'Max userId: {n_users} Max movieId: {n_items}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCF(\n",
       "  (user_emb): Embedding(6040, 32)\n",
       "  (item_emb): Embedding(3706, 32)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = NCF(n_users, n_items)\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(filename=f'{datetime.now().isoformat()}_log.txt', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, epochs, pos_train_dataloader, neg_train_dataloader, test_dataset, lr=0.001):\n",
    "\n",
    "    # Define Binary Cross Entropy (BCE) loss as the criterion\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Define the optimizer (e.g., Adam optimizer)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for iter, (pos_batch, ns_batch) in enumerate(tqdm(zip(pos_train_dataloader, neg_train_dataloader), total=len(pos_train_dataloader), desc=f'Epoch {epoch + 1}/{epochs}')):\n",
    "            user_ids = torch.cat([pos_batch['user_id'], ns_batch['user_id']]).to(device)\n",
    "            movie_ids = torch.cat([pos_batch['item_id'], ns_batch['item_id']]).to(device)\n",
    "            targets = torch.cat([pos_batch['target'], ns_batch['target']]).float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(user_ids, movie_ids)\n",
    "            outputs = torch.squeeze(outputs, dim=1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        average_loss = total_loss / len(pos_train_dataloader)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {average_loss:.4f}\")\n",
    "\n",
    "        # Evaluate on test set after each epoch\n",
    "        evaluate(model, test_dataset, k=10)  # You need to define the evaluate function\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "def evaluate(model, test_dataset, k):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    hr_sum = 0.0\n",
    "    ndcg_sum = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_dataset):\n",
    "            user_ids = data['sample_userId'].to(device).squeeze(1)\n",
    "            movie_ids = data['sample_movieId'].to(device).squeeze(1)\n",
    "            targets = data['targets'].to(device).squeeze(1)\n",
    "\n",
    "            outputs = model(user_ids, movie_ids)\n",
    "            outputs = torch.squeeze(outputs, dim=1)\n",
    "            # Assuming your model outputs probabilities and you want to use BCE loss\n",
    "            loss = nn.BCELoss()(outputs, targets.float())\n",
    "\n",
    "            # Calculate HR@k and NDCG@k\n",
    "            hr, ndcg = calculate_metrics(outputs, targets, k)\n",
    "\n",
    "            hr_sum += hr\n",
    "            ndcg_sum += ndcg\n",
    "            test_loss += loss.detach().cpu().item()\n",
    "\n",
    "    average_hr = hr_sum / len(test_dataset)\n",
    "    average_ndcg = ndcg_sum / len(test_dataset)\n",
    "    average_test_loss = test_loss/len(test_dataset)\n",
    "    print('\\nEvaluation:')\n",
    "    print(f\"Eval Loss: {average_test_loss:.4f} HR@{k}: {average_hr:.4f}, NDCG@{k}: {average_ndcg:.4f}\\n\")\n",
    "\n",
    "def calculate_metrics(outputs, targets, k):\n",
    "    outputs = outputs.squeeze()\n",
    "    targets = targets.squeeze()\n",
    "    \n",
    "    # Get indices where target is 1\n",
    "    positive_indices = torch.nonzero(targets).squeeze(dim=1)\n",
    "\n",
    "    k = min(len(positive_indices), k)\n",
    "    # Get top-k predicted indices for each user\n",
    "    _, indices = torch.topk(outputs, k, dim=0)\n",
    "    \n",
    "    # Create a binary matrix indicating top-k predictions\n",
    "    top_k_binary = torch.zeros(k).to(targets.device)\n",
    "    for i in range(k):\n",
    "        if indices[i] in positive_indices:\n",
    "            top_k_binary[i] = 1\n",
    "    \n",
    "    \n",
    "    # Calculate HR@k\n",
    "    hr_at_k = top_k_binary.sum().item()/k\n",
    "    \n",
    "    # Calculate DCG and IDCG for NDCG@k\n",
    "    dcg = (top_k_binary / torch.log2(torch.arange(2, k + 2).float().to(targets.device))).sum().item()\n",
    "    idcg = (torch.sort(top_k_binary.clone(), dim=0, descending=True)[0]/ torch.log2(torch.arange(2, k + 2).float().to(targets.device))).sum().item()\n",
    "    \n",
    "    # Calculate NDCG@k\n",
    "    if idcg:\n",
    "        ndcg_at_k = dcg / idcg\n",
    "    else:\n",
    "        ndcg_at_k = 0\n",
    "\n",
    "    return hr_at_k, ndcg_at_k\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max userId: 6040 Max movieId: 3706\n"
     ]
    }
   ],
   "source": [
    "n_items = len(ratings['movieId'].unique())\n",
    "n_users = len(ratings['userId'].unique())\n",
    "print(f'Max userId: {n_users} Max movieId: {n_items}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCF(\n",
       "  (user_emb): Embedding(6040, 32)\n",
       "  (item_emb): Embedding(3706, 32)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelNCF = NCF(n_users, n_items)\n",
    "modelNCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_ratio = 5\n",
    "batch_size = 4096\n",
    "pos_train_dataset  = PosDataset(ratings)\n",
    "neg_train_dataset = NegativeSampler(ratings, negative_ratio=negative_ratio, strategy='uniform')\n",
    "test_dateset = TestDataset(ratings, test_ratings)\n",
    "\n",
    "pos_train_dataloader = DataLoader(pos_train_dataset, batch_size=batch_size)\n",
    "neg_train_dataloader = DataLoader(neg_train_dataset, batch_size=negative_ratio*batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/245 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 245/245 [01:42<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.5523\n",
      "\n",
      "Evaluation:\n",
      "Eval Loss: 0.5101 HR@10: 0.2254, NDCG@10: 0.4742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 245/245 [01:41<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.4498\n",
      "\n",
      "Evaluation:\n",
      "Eval Loss: 0.5082 HR@10: 0.2866, NDCG@10: 0.5615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 245/245 [01:40<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.4406\n",
      "\n",
      "Evaluation:\n",
      "Eval Loss: 0.4864 HR@10: 0.4033, NDCG@10: 0.6659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 245/245 [01:41<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.4063\n",
      "\n",
      "Evaluation:\n",
      "Eval Loss: 0.4502 HR@10: 0.4463, NDCG@10: 0.6995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 245/245 [01:40<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.3766\n",
      "\n",
      "Evaluation:\n",
      "Eval Loss: 0.4283 HR@10: 0.4669, NDCG@10: 0.7163\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "train(modelNCF, 5, pos_train_dataloader, neg_train_dataloader, test_dateset, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"assets/NeuMF.png\" alt=\"NCF\" width=\"50%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, user_num, item_num, hidden_dims=[32, 16, 8], log_file='neumf_log.txt'):\n",
    "        super(NeuMF, self).__init__()\n",
    "\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.user_emb = nn.Embedding(num_embeddings=user_num, embedding_dim=2 * hidden_dims[0])\n",
    "        self.item_emb = nn.Embedding(num_embeddings=item_num, embedding_dim=2 * hidden_dims[0])\n",
    "\n",
    "        layers = [nn.Linear(in_features=2 * hidden_dims[0], out_features=hidden_dims[1])]\n",
    "        for i in range(1, len(hidden_dims) - 1):\n",
    "            layers.extend([nn.Tanh(), nn.Linear(in_features=hidden_dims[i], out_features=hidden_dims[i + 1])])\n",
    "        layers.append(nn.Tanh())\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.neumf = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_dims[0] + hidden_dims[-1], out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        \n",
    "        user_emb = self.user_emb(user_ids)\n",
    "        user_emb_mf, user_emb_mlp = user_emb[:, :self.hidden_dims[0]], user_emb[:, self.hidden_dims[0]:]\n",
    "\n",
    "        item_emb = self.item_emb(item_ids)\n",
    "        item_emb_mf, item_emb_mlp = item_emb[:, :self.hidden_dims[0]], item_emb[:, self.hidden_dims[0]:]\n",
    "\n",
    "        emb_mf = user_emb_mf * item_emb_mf\n",
    "\n",
    "        emb_mlp = torch.cat([user_emb_mlp, item_emb_mlp], dim=1)\n",
    "        out_mlp = self.mlp(emb_mlp)\n",
    "\n",
    "        out = torch.cat([emb_mf, out_mlp], dim=1)\n",
    "        out = self.neumf(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuMF(\n",
       "  (user_emb): Embedding(6040, 64)\n",
       "  (item_emb): Embedding(3706, 64)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       "  (neumf): Sequential(\n",
       "    (0): Linear(in_features=40, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelNeuMF = NeuMF(n_users, n_items)\n",
    "modelNeuMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/245 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 245/245 [01:41<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.5342\n",
      "\n",
      "Evaluation:\n",
      "Eval Loss: 0.4583 HR@10: 0.5178, NDCG@10: 0.7548\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 245/245 [01:42<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.3737\n",
      "\n",
      "Evaluation:\n",
      "Eval Loss: 0.4259 HR@10: 0.5500, NDCG@10: 0.7706\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 245/245 [01:41<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.3512\n",
      "\n",
      "Evaluation:\n",
      "Eval Loss: 0.4098 HR@10: 0.5505, NDCG@10: 0.7706\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 245/245 [01:42<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.3380\n",
      "\n",
      "Evaluation:\n",
      "Eval Loss: 0.3956 HR@10: 0.5515, NDCG@10: 0.7709\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 245/245 [01:40<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.3262\n",
      "\n",
      "Evaluation:\n",
      "Eval Loss: 0.3839 HR@10: 0.5503, NDCG@10: 0.7702\n",
      "\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "train(modelNeuMF, 5, pos_train_dataloader, neg_train_dataloader, test_dateset, lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
